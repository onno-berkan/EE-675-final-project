{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = \"/Users/apple/Documents/MATLAB/EE 675/Willett Data/tuning-tasks-all/\"\n",
    "import scipy.io\n",
    "\n",
    "fiftyWordDat = scipy.io.loadmat(baseDir+'tuningTasks/t12.2022.05.03_fiftyWordSet.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fiftyWordDat['feat'] = np.concatenate([fiftyWordDat['tx2'][:,:32].astype(np.float32), fiftyWordDat['tx2'][:,96:128].astype(np.float32), fiftyWordDat['spikePow'][:,:32].astype(np.float32), fiftyWordDat['spikePow'][:,96:128].astype(np.float32)], axis=1)\n",
    "fiftyWordDat['feat'] = np.sqrt(fiftyWordDat['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an array containing the data\n",
    "\n",
    "fiftyWordSubset = []\n",
    "labels = []\n",
    "\n",
    "for cue in fiftyWordDat['cueList'][0]:\n",
    "    cueIdx = np.where(fiftyWordDat['cueList'] == cue)[1]\n",
    "    cueTrials = np.where(fiftyWordDat['trialCues'] == cueIdx)[0]\n",
    "    cueTrialEpochs = [fiftyWordDat['goTrialEpochs'][trialNum] for trialNum in cueTrials]\n",
    "    cueTrialBins = [fiftyWordDat['feat'][epoch[1] - 50:epoch[1]] for epoch in cueTrialEpochs] # the last 50 bins were found to be the most informative\n",
    "\n",
    "    fiftyWordSubset.append(cueTrialBins)\n",
    "    labels.append(np.ones(20)*cueIdx)\n",
    "\n",
    "fiftyWordSubset = np.concatenate(fiftyWordSubset[1:], axis=0) # (1000 trials, 50 bins, 128 channels) array\n",
    "labels = np.concatenate(labels[:50]) # (1000,) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "from pykalman import KalmanFilter\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# global 'meta' KF\u001b[39;00m\n\u001b[32m     16\u001b[39m kf_global = KalmanFilter(\n\u001b[32m     17\u001b[39m     n_dim_state=LATENT_DIM,\n\u001b[32m     18\u001b[39m     n_dim_obs=\u001b[32m128\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m              \u001b[33m'\u001b[39m\u001b[33minitial_state_mean\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minitial_state_covariance\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     23\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m kf_global = \u001b[43mkf_global\u001b[49m\u001b[43m.\u001b[49m\u001b[43mem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_masked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 10 iters is enough\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGlobal Model Trained. Extracting Features...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# takes 40+ minutes with 10/12 latent dimensions\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# takes 303 minutes with 16 dims\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/675-final-project/lib/python3.11/site-packages/pykalman/standard.py:1486\u001b[39m, in \u001b[36mKalmanFilter.em\u001b[39m\u001b[34m(self, X, y, n_iter, em_vars)\u001b[39m\n\u001b[32m   1478\u001b[39m \u001b[38;5;66;03m# Actual EM iterations\u001b[39;00m\n\u001b[32m   1479\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[32m   1480\u001b[39m     (\n\u001b[32m   1481\u001b[39m         predicted_state_means,\n\u001b[32m   1482\u001b[39m         predicted_state_covariances,\n\u001b[32m   1483\u001b[39m         kalman_gains,\n\u001b[32m   1484\u001b[39m         filtered_state_means,\n\u001b[32m   1485\u001b[39m         filtered_state_covariances,\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m     ) = \u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransition_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransition_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransition_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitial_state_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitial_state_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1497\u001b[39m     (\n\u001b[32m   1498\u001b[39m         smoothed_state_means,\n\u001b[32m   1499\u001b[39m         smoothed_state_covariances,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1506\u001b[39m         predicted_state_covariances,\n\u001b[32m   1507\u001b[39m     )\n\u001b[32m   1508\u001b[39m     sigma_pair_smooth = _smooth_pair(\n\u001b[32m   1509\u001b[39m         smoothed_state_covariances, kalman_smoothing_gains\n\u001b[32m   1510\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/675-final-project/lib/python3.11/site-packages/pykalman/standard.py:402\u001b[39m, in \u001b[36m_filter\u001b[39m\u001b[34m(transition_matrices, observation_matrices, transition_covariance, observation_covariance, transition_offsets, observation_offsets, initial_state_mean, initial_state_covariance, observations)\u001b[39m\n\u001b[32m    396\u001b[39m     observation_covariance_t = _last_dims(observation_covariance, t)\n\u001b[32m    397\u001b[39m     observation_offset = _last_dims(observation_offsets, t, ndims=\u001b[32m1\u001b[39m)\n\u001b[32m    398\u001b[39m     (\n\u001b[32m    399\u001b[39m         kalman_gains[t],\n\u001b[32m    400\u001b[39m         filtered_state_means[t],\n\u001b[32m    401\u001b[39m         filtered_state_covariances[t],\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     ) = \u001b[43m_filter_correct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservation_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservation_covariance_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservation_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicted_state_means\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicted_state_covariances\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    412\u001b[39m     predicted_state_means,\n\u001b[32m    413\u001b[39m     predicted_state_covariances,\n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m     filtered_state_covariances,\n\u001b[32m    417\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/675-final-project/lib/python3.11/site-packages/pykalman/standard.py:288\u001b[39m, in \u001b[36m_filter_correct\u001b[39m\u001b[34m(observation_matrix, observation_covariance, observation_offset, predicted_state_mean, predicted_state_covariance, observation)\u001b[39m\n\u001b[32m    275\u001b[39m predicted_observation_mean = (\n\u001b[32m    276\u001b[39m     np.dot(observation_matrix, predicted_state_mean) + observation_offset\n\u001b[32m    277\u001b[39m )\n\u001b[32m    278\u001b[39m predicted_observation_covariance = (\n\u001b[32m    279\u001b[39m     np.dot(\n\u001b[32m    280\u001b[39m         observation_matrix,\n\u001b[32m   (...)\u001b[39m\u001b[32m    283\u001b[39m     + observation_covariance\n\u001b[32m    284\u001b[39m )\n\u001b[32m    286\u001b[39m kalman_gain = np.dot(\n\u001b[32m    287\u001b[39m     predicted_state_covariance,\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     np.dot(observation_matrix.T, \u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_observation_covariance\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[32m    289\u001b[39m )\n\u001b[32m    291\u001b[39m corrected_state_mean = predicted_state_mean + np.dot(\n\u001b[32m    292\u001b[39m     kalman_gain, observation - predicted_observation_mean\n\u001b[32m    293\u001b[39m )\n\u001b[32m    294\u001b[39m corrected_state_covariance = predicted_state_covariance - np.dot(\n\u001b[32m    295\u001b[39m     kalman_gain, np.dot(observation_matrix, predicted_state_covariance)\n\u001b[32m    296\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/675-final-project/lib/python3.11/site-packages/scipy/_lib/_util.py:1233\u001b[39m, in \u001b[36m_apply_over_batch.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m \u001b[38;5;66;03m# Early exit if call is not batched\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(batch_shapes):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;66;03m# Determine broadcasted batch shape\u001b[39;00m\n\u001b[32m   1236\u001b[39m batch_shape = np.broadcast_shapes(*batch_shapes)  \u001b[38;5;66;03m# Gives OK error message\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/675-final-project/lib/python3.11/site-packages/scipy/linalg/_basic.py:1633\u001b[39m, in \u001b[36mpinv\u001b[39m\u001b[34m(a, atol, rtol, return_rank, check_finite)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mCompute the (Moore-Penrose) pseudo-inverse of a matrix.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1630\u001b[39m \n\u001b[32m   1631\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1632\u001b[39m a = _asarray_validated(a, check_finite=check_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1633\u001b[39m u, s, vh = \u001b[43m_decomp_svd\u001b[49m\u001b[43m.\u001b[49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1634\u001b[39m t = u.dtype.char.lower()\n\u001b[32m   1635\u001b[39m maxS = np.max(s, initial=\u001b[32m0.\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/675-final-project/lib/python3.11/site-packages/scipy/_lib/_util.py:1233\u001b[39m, in \u001b[36m_apply_over_batch.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m \u001b[38;5;66;03m# Early exit if call is not batched\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(batch_shapes):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;66;03m# Determine broadcasted batch shape\u001b[39;00m\n\u001b[32m   1236\u001b[39m batch_shape = np.broadcast_shapes(*batch_shapes)  \u001b[38;5;66;03m# Gives OK error message\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/675-final-project/lib/python3.11/site-packages/scipy/linalg/_decomp_svd.py:166\u001b[39m, in \u001b[36msvd\u001b[39m\u001b[34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[39m\n\u001b[32m    162\u001b[39m lwork = _compute_lwork(gesXd_lwork, a1.shape[\u001b[32m0\u001b[39m], a1.shape[\u001b[32m1\u001b[39m],\n\u001b[32m    163\u001b[39m                        compute_uv=compute_uv, full_matrices=full_matrices)\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# perform decomposition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m u, s, v, info = \u001b[43mgesXd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info > \u001b[32m0\u001b[39m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[33m\"\u001b[39m\u001b[33mSVD did not converge\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "X_with_nans = []\n",
    "for trial in fiftyWordSubset:\n",
    "    X_with_nans.append(trial)\n",
    "    X_with_nans.append(np.full((1, 128), np.nan))\n",
    "\n",
    "X_stacked = np.vstack(X_with_nans)[:-1]\n",
    "X_masked = ma.masked_invalid(X_stacked)\n",
    "\n",
    "# factor analysis\n",
    "fa = FactorAnalysis(n_components=LATENT_DIM)\n",
    "fa.fit(fiftyWordSubset.reshape(-1, 128))\n",
    "C_init = fa.components_.T\n",
    "R_init = np.diag(fa.noise_variance_)\n",
    "\n",
    "# global 'meta' KF\n",
    "kf_global = KalmanFilter(\n",
    "    n_dim_state=LATENT_DIM,\n",
    "    n_dim_obs=128,\n",
    "    observation_matrices=C_init,\n",
    "    observation_covariance=R_init,\n",
    "    em_vars=['transition_matrices', 'transition_covariance', \n",
    "             'initial_state_mean', 'initial_state_covariance']\n",
    ")\n",
    "kf_global = kf_global.em(X_masked, n_iter=1) # 10 iters is enough\n",
    "\n",
    "print(\"Global Model Trained. Extracting Features...\")\n",
    "# Make sure you run this on CARC!!! takes 40+ minutes with 10/12 latent dimensions, takes 303 minutes with 16 dims on your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (1000, 80)\n"
     ]
    }
   ],
   "source": [
    "# feature extraction step\n",
    "def extract_features(data, kf_model):\n",
    "    features = []\n",
    "    for trial in data:\n",
    "        (smoothed, _) = kf_model.smooth(trial)\n",
    "        \n",
    "        # split word into windows -- could be adapted in the future to work on live data\n",
    "        n_windows = 5\n",
    "        window_size = 50 // n_windows\n",
    "        \n",
    "        trial_feats = []\n",
    "        for w in range(n_windows):\n",
    "            # Mean of latent state in this window\n",
    "            window_mean = np.mean(smoothed[w*window_size : (w+1)*window_size], axis=0)\n",
    "            trial_feats.append(window_mean)\n",
    "            \n",
    "        features.append(np.concatenate(trial_feats))\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "# extract features\n",
    "X_latent = extract_features(fiftyWordSubset, kf_global)\n",
    "print(f\"Feature Matrix Shape: {X_latent.shape}\") # (1000, 5*latent_dim_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 48.0%\n",
      "Fold 2 Accuracy: 44.0%\n",
      "Fold 3 Accuracy: 39.5%\n",
      "Fold 4 Accuracy: 43.0%\n",
      "Fold 5 Accuracy: 41.5%\n",
      "\n",
      "Chance Level: 2%\n",
      "\n",
      "Average Accuracy: 43.2%\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42) # 5 folds -- gets slightly better acc at 10\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# Ensure y is 1D\n",
    "y = labels.ravel().astype(int)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_latent, y)):\n",
    "\n",
    "    X_train, X_test = X_latent[train_idx], X_latent[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    lda.fit(X_train, y_train)\n",
    "    \n",
    "    pred = lda.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    print(f\"Fold {fold+1} Accuracy: {acc*100:.1f}%\")\n",
    "\n",
    "print(\"\\nChance Level: 2%\")\n",
    "print(f\"\\nAverage Accuracy: {np.mean(accuracies)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the bins for yes and no trials\n",
    "\n",
    "yesIdx = np.where(fiftyWordDat['cueList'] == 'yes')[1][0]\n",
    "noIdx = np.where(fiftyWordDat['cueList'] == 'no')[1][0]\n",
    "\n",
    "noTrials = np.where(fiftyWordDat['trialCues'] == noIdx)[0]\n",
    "yesTrials = np.where(fiftyWordDat['trialCues'] == yesIdx)[0]\n",
    "\n",
    "noTrialEpochs = [fiftyWordDat['goTrialEpochs'][trialNum] for trialNum in noTrials]\n",
    "yesTrialEpochs = [fiftyWordDat['goTrialEpochs'][trialNum] for trialNum in yesTrials]\n",
    "\n",
    "noTrialBins = [fiftyWordDat['feat'][epoch[1] - 50:epoch[1]] for epoch in noTrialEpochs] # the last 50 bins were found to be the most informative\n",
    "yesTrialBins = [fiftyWordDat['feat'][epoch[1] - 50:epoch[1]] for epoch in yesTrialEpochs] # (20, 50, 256) \n",
    "\n",
    "yesNoData = np.concatenate((noTrialBins, yesTrialBins))\n",
    "yesNoLabels = np.concatenate((np.zeros(20),np.ones(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Model Trained. Extracting Features...\n"
     ]
    }
   ],
   "source": [
    "X_with_nans = []\n",
    "for trial in yesNoData:\n",
    "    X_with_nans.append(trial)\n",
    "    X_with_nans.append(np.full((1, 128), np.nan))\n",
    "\n",
    "X_stacked = np.vstack(X_with_nans)[:-1]\n",
    "X_masked = ma.masked_invalid(X_stacked)\n",
    "\n",
    "# factor analysis\n",
    "fa = FactorAnalysis(n_components=LATENT_DIM)\n",
    "fa.fit(yesNoData.reshape(-1, 128))\n",
    "C_init = fa.components_.T\n",
    "R_init = np.diag(fa.noise_variance_)\n",
    "\n",
    "# global 'meta' KF\n",
    "kf_global = KalmanFilter(\n",
    "    n_dim_state=LATENT_DIM,\n",
    "    n_dim_obs=128,\n",
    "    observation_matrices=C_init,\n",
    "    observation_covariance=R_init,\n",
    "    em_vars=['transition_matrices', 'transition_covariance', \n",
    "             'initial_state_mean', 'initial_state_covariance']\n",
    ")\n",
    "kf_global = kf_global.em(X_masked, n_iter=10) # 10 iters is enough\n",
    "\n",
    "print(\"Global Model Trained. Extracting Features...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (40, 60)\n"
     ]
    }
   ],
   "source": [
    "# feature extraction step\n",
    "def extract_features(data, kf_model):\n",
    "    features = []\n",
    "    for trial in data:\n",
    "        (smoothed, _) = kf_model.smooth(trial)\n",
    "        \n",
    "        # split word into windows -- could be adapted in the future to work on live data\n",
    "        n_windows = 5\n",
    "        window_size = 50 // n_windows\n",
    "        \n",
    "        trial_feats = []\n",
    "        for w in range(n_windows):\n",
    "            # Mean of latent state in this window\n",
    "            window_mean = np.mean(smoothed[w*window_size : (w+1)*window_size], axis=0)\n",
    "            trial_feats.append(window_mean)\n",
    "            \n",
    "        features.append(np.concatenate(trial_feats))\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "# extract features\n",
    "X_latent = extract_features(yesNoData, kf_global)\n",
    "print(f\"Feature Matrix Shape: {X_latent.shape}\") # (1000, 5*latent_dim_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 87.5%\n",
      "Fold 2 Accuracy: 100.0%\n",
      "Fold 3 Accuracy: 100.0%\n",
      "Fold 4 Accuracy: 87.5%\n",
      "Fold 5 Accuracy: 87.5%\n",
      "\n",
      "Chance Level: 2%\n",
      "\n",
      "Average Accuracy: 92.5%\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # 5 folds -- gets slightly better acc at 10\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "y = yesNoLabels.ravel().astype(int)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_latent, y)):\n",
    "    X_train, X_test = X_latent[train_idx], X_latent[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    lda.fit(X_train, y_train)\n",
    "    \n",
    "    pred = lda.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    print(f\"Fold {fold+1} Accuracy: {acc*100:.1f}%\")\n",
    "\n",
    "print(\"\\nChance Level: 2%\")\n",
    "print(f\"\\nAverage Accuracy: {np.mean(accuracies)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir2 = \"/Users/apple/Documents/MATLAB/EE 675/Willett Data/sentences/\"\n",
    "\n",
    "compDat1 = scipy.io.loadmat(baseDir2+'t12.2022.05.19_sentences.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "compDat1['feat'] = np.concatenate([compDat1['tx2'][:,:32].astype(np.float32), compDat1['tx2'][:,96:128].astype(np.float32), compDat1['spikePow'][:,:32].astype(np.float32), compDat1['spikePow'][:,96:128].astype(np.float32)], axis=1)\n",
    "compDat1['feat'] = np.sqrt(compDat1['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockList = compDat1['blockList'][np.where(compDat1['blockTypes'] == 'OL Chang')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changSentences(data, blockList):\n",
    "    sentences = []\n",
    "    binRange = []\n",
    "    for block in blockList:\n",
    "        rangeTemp = np.where(data['blockNum'] == block)[0]\n",
    "        trialMin = np.where(data['goTrialEpochs'] == rangeTemp[0])[0][0] + 1\n",
    "        trialMax = np.where(data['goTrialEpochs'] == rangeTemp[-1] + 1)[0][0]\n",
    "        sentences.append(data['sentences'][trialMin:trialMax + 1])\n",
    "        binRange.append(data['goTrialEpochs'][trialMin:trialMax + 1])\n",
    "    return np.concatenate(np.squeeze(sentences)), np.concatenate(binRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDayN(data):\n",
    "    sentenceList, binRange = changSentences(data, blockList)\n",
    "\n",
    "    yesIdx = np.where(sentenceList == 'Yes')[0][0]\n",
    "    noIdx = np.where(sentenceList == 'No')[0][0]\n",
    "\n",
    "    noTrialBins = data['feat'][binRange[noIdx, 1] - 50:binRange[noIdx, 1]] # the last 50 bins were found to be the most informative\n",
    "    yesTrialBins = data['feat'][binRange[yesIdx, 1] - 50:binRange[yesIdx, 1]]\n",
    "\n",
    "    return noTrialBins, yesTrialBins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "noTrialBins, yesTrialBins = normalizeDayN(compDat1)\n",
    "otherDayYesNo = np.array([noTrialBins, yesTrialBins])\n",
    "otherDayLabels = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for trial in otherDayYesNo:\n",
    "    (smoothed, _) = kf_global.smooth(trial)\n",
    "    \n",
    "    # split word into windows -- could be adapted in the future to work on live data\n",
    "    n_windows = 5\n",
    "    window_size = 50 // n_windows\n",
    "    \n",
    "    trial_feats = []\n",
    "    for w in range(n_windows):\n",
    "        # Mean of latent state in this window\n",
    "        window_mean = np.mean(smoothed[w*window_size : (w+1)*window_size], axis=0)\n",
    "        trial_feats.append(window_mean)\n",
    "        \n",
    "    features.append(np.concatenate(trial_feats))\n",
    "\n",
    "otherDayFeatures = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chance Level: 50%\n",
      "\n",
      "Average Accuracy: 60.6%\n"
     ]
    }
   ],
   "source": [
    "pred = lda.predict(otherDayFeatures)\n",
    "acc = accuracy_score(otherDayLabels, pred)\n",
    "accuracies.append(acc)\n",
    "\n",
    "print(\"\\nChance Level: 50%\")\n",
    "print(f\"\\nAverage Accuracy: {np.mean(accuracies)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix accuracy above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "675-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
