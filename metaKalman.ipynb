{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = \"/Users/apple/Documents/MATLAB/EE 675/Willett Data/tuning-tasks-all/\"\n",
    "import scipy.io\n",
    "\n",
    "fiftyWordDat = scipy.io.loadmat(baseDir+'tuningTasks/t12.2022.05.03_fiftyWordSet.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fiftyWordDat['feat'] = np.concatenate([fiftyWordDat['tx2'][:,:32].astype(np.float32), fiftyWordDat['tx2'][:,96:128].astype(np.float32), fiftyWordDat['spikePow'][:,:32].astype(np.float32), fiftyWordDat['spikePow'][:,96:128].astype(np.float32)], axis=1)\n",
    "fiftyWordDat['feat'] = np.sqrt(fiftyWordDat['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an array containing the data\n",
    "\n",
    "fiftyWordSubset = []\n",
    "labels = []\n",
    "\n",
    "for cue in fiftyWordDat['cueList'][0]:\n",
    "    cueIdx = np.where(fiftyWordDat['cueList'] == cue)[1]\n",
    "    cueTrials = np.where(fiftyWordDat['trialCues'] == cueIdx)[0]\n",
    "    cueTrialEpochs = [fiftyWordDat['goTrialEpochs'][trialNum] for trialNum in cueTrials]\n",
    "    cueTrialBins = [fiftyWordDat['feat'][epoch[1] - 50:epoch[1]] for epoch in cueTrialEpochs] # the last 50 bins were found to be the most informative\n",
    "\n",
    "    fiftyWordSubset.append(cueTrialBins)\n",
    "    labels.append(np.ones(20)*cueIdx)\n",
    "\n",
    "fiftyWordSubset = np.concatenate(fiftyWordSubset[1:], axis=0) # (1000 trials, 50 bins, 128 channels) array\n",
    "labels = np.concatenate(labels[:50]) # (1000,) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "from pykalman import KalmanFilter\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Model Trained. Extracting Features...\n"
     ]
    }
   ],
   "source": [
    "X_with_nans = []\n",
    "for trial in fiftyWordSubset:\n",
    "    X_with_nans.append(trial)\n",
    "    X_with_nans.append(np.full((1, 128), np.nan))\n",
    "\n",
    "X_stacked = np.vstack(X_with_nans)[:-1]\n",
    "X_masked = ma.masked_invalid(X_stacked)\n",
    "\n",
    "# factor analysis\n",
    "fa = FactorAnalysis(n_components=LATENT_DIM)\n",
    "fa.fit(fiftyWordSubset.reshape(-1, 128))\n",
    "C_init = fa.components_.T\n",
    "R_init = np.diag(fa.noise_variance_)\n",
    "\n",
    "# global 'meta' KF\n",
    "kf_global = KalmanFilter(\n",
    "    n_dim_state=LATENT_DIM,\n",
    "    n_dim_obs=128,\n",
    "    observation_matrices=C_init,\n",
    "    observation_covariance=R_init,\n",
    "    em_vars=['transition_matrices', 'transition_covariance', \n",
    "             'initial_state_mean', 'initial_state_covariance']\n",
    ")\n",
    "kf_global = kf_global.em(X_masked, n_iter=10) # 10 iters is enough\n",
    "\n",
    "print(\"Global Model Trained. Extracting Features...\")\n",
    "# takes 40+ minutes with 10/12 latent dimensions\n",
    "# takes 303 minutes with 16 dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (1000, 80)\n"
     ]
    }
   ],
   "source": [
    "# feature extraction step\n",
    "def extract_features(data, kf_model):\n",
    "    features = []\n",
    "    for trial in data:\n",
    "        (smoothed, _) = kf_model.smooth(trial)\n",
    "        \n",
    "        # split word into windows -- could be adapted in the future to work on live data\n",
    "        n_windows = 5\n",
    "        window_size = 50 // n_windows\n",
    "        \n",
    "        trial_feats = []\n",
    "        for w in range(n_windows):\n",
    "            # Mean of latent state in this window\n",
    "            window_mean = np.mean(smoothed[w*window_size : (w+1)*window_size], axis=0)\n",
    "            trial_feats.append(window_mean)\n",
    "            \n",
    "        features.append(np.concatenate(trial_feats))\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "# extract features\n",
    "X_latent = extract_features(fiftyWordSubset, kf_global)\n",
    "print(f\"Feature Matrix Shape: {X_latent.shape}\") # (1000, 5*latent_dim_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 48.0%\n",
      "Fold 2 Accuracy: 44.0%\n",
      "Fold 3 Accuracy: 39.5%\n",
      "Fold 4 Accuracy: 43.0%\n",
      "Fold 5 Accuracy: 41.5%\n",
      "\n",
      "Chance Level: 2%\n",
      "\n",
      "Average Accuracy: 43.2%\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # 5 folds -- gets slightly better acc at 10\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# Ensure y is 1D\n",
    "y = labels.ravel().astype(int)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_latent, y)):\n",
    "    X_train, X_test = X_latent[train_idx], X_latent[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    lda.fit(X_train, y_train)\n",
    "    \n",
    "    pred = lda.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    print(f\"Fold {fold+1} Accuracy: {acc*100:.1f}%\")\n",
    "\n",
    "print(\"\\nChance Level: 2%\")\n",
    "print(f\"\\nAverage Accuracy: {np.mean(accuracies)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the bins for yes and no trials\n",
    "\n",
    "yesIdx = np.where(fiftyWordDat['cueList'] == 'yes')[1][0]\n",
    "noIdx = np.where(fiftyWordDat['cueList'] == 'no')[1][0]\n",
    "\n",
    "noTrials = np.where(fiftyWordDat['trialCues'] == noIdx)[0]\n",
    "yesTrials = np.where(fiftyWordDat['trialCues'] == yesIdx)[0]\n",
    "\n",
    "noTrialEpochs = [fiftyWordDat['goTrialEpochs'][trialNum] for trialNum in noTrials]\n",
    "yesTrialEpochs = [fiftyWordDat['goTrialEpochs'][trialNum] for trialNum in yesTrials]\n",
    "\n",
    "noTrialBins = [fiftyWordDat['feat'][epoch[1] - 50:epoch[1]] for epoch in noTrialEpochs] # the last 50 bins were found to be the most informative\n",
    "yesTrialBins = [fiftyWordDat['feat'][epoch[1] - 50:epoch[1]] for epoch in yesTrialEpochs] # (20, 50, 256) \n",
    "\n",
    "yesNoData = np.concatenate((noTrialBins, yesTrialBins))\n",
    "yesNoLabels = np.concatenate((np.zeros(20),np.ones(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Model Trained. Extracting Features...\n"
     ]
    }
   ],
   "source": [
    "X_with_nans = []\n",
    "for trial in yesNoData:\n",
    "    X_with_nans.append(trial)\n",
    "    X_with_nans.append(np.full((1, 128), np.nan))\n",
    "\n",
    "X_stacked = np.vstack(X_with_nans)[:-1]\n",
    "X_masked = ma.masked_invalid(X_stacked)\n",
    "\n",
    "# factor analysis\n",
    "fa = FactorAnalysis(n_components=LATENT_DIM)\n",
    "fa.fit(yesNoData.reshape(-1, 128))\n",
    "C_init = fa.components_.T\n",
    "R_init = np.diag(fa.noise_variance_)\n",
    "\n",
    "# global 'meta' KF\n",
    "kf_global = KalmanFilter(\n",
    "    n_dim_state=LATENT_DIM,\n",
    "    n_dim_obs=128,\n",
    "    observation_matrices=C_init,\n",
    "    observation_covariance=R_init,\n",
    "    em_vars=['transition_matrices', 'transition_covariance', \n",
    "             'initial_state_mean', 'initial_state_covariance']\n",
    ")\n",
    "kf_global = kf_global.em(X_masked, n_iter=10) # 10 iters is enough\n",
    "\n",
    "print(\"Global Model Trained. Extracting Features...\")\n",
    "# takes 40+ minutes with 10+ latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (40, 60)\n"
     ]
    }
   ],
   "source": [
    "# feature extraction step\n",
    "def extract_features(data, kf_model):\n",
    "    features = []\n",
    "    for trial in data:\n",
    "        (smoothed, _) = kf_model.smooth(trial)\n",
    "        \n",
    "        # split word into windows -- could be adapted in the future to work on live data\n",
    "        n_windows = 5\n",
    "        window_size = 50 // n_windows\n",
    "        \n",
    "        trial_feats = []\n",
    "        for w in range(n_windows):\n",
    "            # Mean of latent state in this window\n",
    "            window_mean = np.mean(smoothed[w*window_size : (w+1)*window_size], axis=0)\n",
    "            trial_feats.append(window_mean)\n",
    "            \n",
    "        features.append(np.concatenate(trial_feats))\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "# extract features\n",
    "X_latent = extract_features(yesNoData, kf_global)\n",
    "print(f\"Feature Matrix Shape: {X_latent.shape}\") # (1000, 5*latent_dim_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 87.5%\n",
      "Fold 2 Accuracy: 100.0%\n",
      "Fold 3 Accuracy: 100.0%\n",
      "Fold 4 Accuracy: 87.5%\n",
      "Fold 5 Accuracy: 87.5%\n",
      "\n",
      "Chance Level: 2%\n",
      "\n",
      "Average Accuracy: 92.5%\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # 5 folds -- gets slightly better acc at 10\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "y = yesNoLabels.ravel().astype(int)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_latent, y)):\n",
    "    X_train, X_test = X_latent[train_idx], X_latent[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    lda.fit(X_train, y_train)\n",
    "    \n",
    "    pred = lda.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    print(f\"Fold {fold+1} Accuracy: {acc*100:.1f}%\")\n",
    "\n",
    "print(\"\\nChance Level: 2%\")\n",
    "print(f\"\\nAverage Accuracy: {np.mean(accuracies)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir2 = \"/Users/apple/Documents/MATLAB/EE 675/Willett Data/sentences/\"\n",
    "\n",
    "compDat1 = scipy.io.loadmat(baseDir2+'t12.2022.05.19_sentences.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "compDat1['feat'] = np.concatenate([compDat1['tx2'][:,:32].astype(np.float32), compDat1['tx2'][:,96:128].astype(np.float32), compDat1['spikePow'][:,:32].astype(np.float32), compDat1['spikePow'][:,96:128].astype(np.float32)], axis=1)\n",
    "compDat1['feat'] = np.sqrt(compDat1['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockList = compDat1['blockList'][np.where(compDat1['blockTypes'] == 'OL Chang')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changSentences(data, blockList):\n",
    "    sentences = []\n",
    "    binRange = []\n",
    "    for block in blockList:\n",
    "        rangeTemp = np.where(data['blockNum'] == block)[0]\n",
    "        trialMin = np.where(data['goTrialEpochs'] == rangeTemp[0])[0][0] + 1\n",
    "        trialMax = np.where(data['goTrialEpochs'] == rangeTemp[-1] + 1)[0][0]\n",
    "        sentences.append(data['sentences'][trialMin:trialMax + 1])\n",
    "        binRange.append(data['goTrialEpochs'][trialMin:trialMax + 1])\n",
    "    return np.concatenate(np.squeeze(sentences)), np.concatenate(binRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDayN(data):\n",
    "    sentenceList, binRange = changSentences(data, blockList)\n",
    "\n",
    "    yesIdx = np.where(sentenceList == 'Yes')[0][0]\n",
    "    noIdx = np.where(sentenceList == 'No')[0][0]\n",
    "\n",
    "    noTrialBins = data['feat'][binRange[noIdx, 1] - 50:binRange[noIdx, 1]] # the last 50 bins were found to be the most informative\n",
    "    yesTrialBins = data['feat'][binRange[yesIdx, 1] - 50:binRange[yesIdx, 1]]\n",
    "\n",
    "    return noTrialBins, yesTrialBins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "noTrialBins, yesTrialBins = normalizeDayN(compDat1)\n",
    "otherDayYesNo = np.array([noTrialBins, yesTrialBins])\n",
    "otherDayLabels = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for trial in otherDayYesNo:\n",
    "    (smoothed, _) = kf_global.smooth(trial)\n",
    "    \n",
    "    # split word into windows -- could be adapted in the future to work on live data\n",
    "    n_windows = 5\n",
    "    window_size = 50 // n_windows\n",
    "    \n",
    "    trial_feats = []\n",
    "    for w in range(n_windows):\n",
    "        # Mean of latent state in this window\n",
    "        window_mean = np.mean(smoothed[w*window_size : (w+1)*window_size], axis=0)\n",
    "        trial_feats.append(window_mean)\n",
    "        \n",
    "    features.append(np.concatenate(trial_feats))\n",
    "\n",
    "otherDayFeatures = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chance Level: 50%\n",
      "\n",
      "Average Accuracy: 60.6%\n"
     ]
    }
   ],
   "source": [
    "pred = lda.predict(otherDayFeatures)\n",
    "acc = accuracy_score(otherDayLabels, pred)\n",
    "accuracies.append(acc)\n",
    "\n",
    "print(\"\\nChance Level: 50%\")\n",
    "print(f\"\\nAverage Accuracy: {np.mean(accuracies)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix accuracy above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "675-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
